{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1-xWh4XEKaAbSkHH131Wvf3v_mx5f_ykV","authorship_tag":"ABX9TyMCM+7Pb/POK4396cy1P1xu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"1fVsAhf7iGe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import f1_score\n","from gensim.models import Word2Vec, KeyedVectors\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"D-JodJ4TkMIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Load Dataset\n","# --------------------------\n","# Replace this with your actual dataset\n","data = pd.read_csv(\"/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/Cleaned_dataset.csv\")  # ['final_cleaned_text', 'Sentiments']\n","X = data['final_cleaned_text'].astype(str)\n","X.fillna('too', inplace=True)\n","y = data['Sentiment'].astype('category').cat.codes  # encode to 0,1,2\n","\n","# --------------------------\n","# Helper: Macro F1 calculation\n","# --------------------------\n","def macro_f1(y_true, y_pred):\n","    return f1_score(y_true, y_pred, average='macro')\n","\n","results = []"],"metadata":{"id":"tVTrjYbYkPZn","executionInfo":{"status":"ok","timestamp":1755074282325,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mr. Ashwani Balyan (SU Training Delivery Manager)","userId":"10495710902595717956"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 2. BOW + GBoost\n","# --------------------------\n","vectorizer = CountVectorizer()\n","X_bow = vectorizer.fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n","\n","gb_model = GradientBoostingClassifier()\n","gb_model.fit(X_train, y_train)\n","y_pred = gb_model.predict(X_test)\n","f1_bow = macro_f1(y_test, y_pred)\n","results.append([\"BOW\", \"GBoost\", f1_bow])\n","\n","# --------------------------\n","# 3. Word2Vec (Skip-gram) + LSTM\n","# --------------------------\n","sentences = [text.split() for text in X]\n","w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","sequences = tokenizer.texts_to_sequences(X)\n","word_index = tokenizer.word_index\n","X_seq = pad_sequences(sequences, maxlen=100)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n","\n","model_w2v = Sequential()\n","model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n","model_w2v.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model_w2v.add(Dense(3, activation='softmax'))\n","model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n","f1_w2v = macro_f1(y_test, y_pred)\n","results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n","\n","# --------------------------\n","# 4. GloVe (100d) + LSTM\n","# --------------------------\n","glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n","glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in glove_model:\n","        embedding_matrix[i] = glove_model[word]\n","\n","model_glove = Sequential()\n","model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n","model_glove.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model_glove.add(Dense(3, activation='softmax'))\n","model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n","f1_glove = macro_f1(y_test, y_pred)\n","results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n","\n","# --------------------------\n","# 5. Comparison Table\n","# --------------------------\n","df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n","print(df_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7ltfaTqhuzr","executionInfo":{"status":"ok","timestamp":1755074359274,"user_tz":-330,"elapsed":72217,"user":{"displayName":"Mr. Ashwani Balyan (SU Training Delivery Manager)","userId":"10495710902595717956"}},"outputId":"c17768d2-1035-4eac-bbe7-590057dbe050"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 0.8663\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.5203\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.5303\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 0.5751\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 0.5722\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78aba059c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.7643\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.5370\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - loss: 0.5237\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - loss: 0.5200\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 148ms/step - loss: 0.4253\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n","  Representation Technique Model Type  Macro F1 Score\n","0                      BOW     GBoost        0.539407\n","1     Word2Vec (Skip-gram)       LSTM        0.309333\n","2             GloVe (100d)       LSTM        0.418399\n"]}]},{"cell_type":"markdown","source":["1. Fine-tune the embeddings"],"metadata":{"id":"LYc8mEsokHCz"}},{"cell_type":"code","source":["# --------------------------\n","# 2. BOW + GBoost\n","# --------------------------\n","vectorizer = CountVectorizer()\n","X_bow = vectorizer.fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n","\n","gb_model = GradientBoostingClassifier()\n","gb_model.fit(X_train, y_train)\n","y_pred = gb_model.predict(X_test)\n","f1_bow = macro_f1(y_test, y_pred)\n","results.append([\"BOW\", \"GBoost\", f1_bow])\n","\n","# --------------------------\n","# 3. Word2Vec (Skip-gram) + LSTM\n","# --------------------------\n","sentences = [text.split() for text in X]\n","w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","sequences = tokenizer.texts_to_sequences(X)\n","word_index = tokenizer.word_index\n","X_seq = pad_sequences(sequences, maxlen=100)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n","\n","model_w2v = Sequential()\n","model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n","model_w2v.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model_w2v.add(Dense(3, activation='softmax'))\n","model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n","f1_w2v = macro_f1(y_test, y_pred)\n","results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n","\n","# --------------------------\n","# 4. GloVe (100d) + LSTM\n","# --------------------------\n","glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n","glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in glove_model:\n","        embedding_matrix[i] = glove_model[word]\n","\n","model_glove = Sequential()\n","model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n","model_glove.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model_glove.add(Dense(3, activation='softmax'))\n","model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n","f1_glove = macro_f1(y_test, y_pred)\n","results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n","\n","# --------------------------\n","# 5. Comparison Table\n","# --------------------------\n","df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n","print(df_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ll4NaAhbkIzl","executionInfo":{"status":"ok","timestamp":1755074466789,"user_tz":-330,"elapsed":60528,"user":{"displayName":"Mr. Ashwani Balyan (SU Training Delivery Manager)","userId":"10495710902595717956"}},"outputId":"e9ea113b-b166-429f-e8c0-5fe9dc442e33"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 0.8000\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.5314\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.5484\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.4896\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.4107\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - loss: 0.7959\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.5188\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.5137\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.4680\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.4395\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n","  Representation Technique Model Type  Macro F1 Score\n","0                      BOW     GBoost        0.539407\n","1     Word2Vec (Skip-gram)       LSTM        0.309333\n","2             GloVe (100d)       LSTM        0.418399\n","3                      BOW     GBoost        0.571731\n","4     Word2Vec (Skip-gram)       LSTM        0.384235\n","5             GloVe (100d)       LSTM        0.343494\n"]}]},{"cell_type":"markdown","source":["2. Use Bidirectional LSTM + Dense layers"],"metadata":{"id":"S1bCIT6smFPL"}},{"cell_type":"code","source":["# --------------------------\n","# 2. BOW + GBoost\n","# --------------------------\n","vectorizer = CountVectorizer()\n","X_bow = vectorizer.fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n","\n","gb_model = GradientBoostingClassifier()\n","gb_model.fit(X_train, y_train)\n","y_pred = gb_model.predict(X_test)\n","f1_bow = macro_f1(y_test, y_pred)\n","results.append([\"BOW\", \"GBoost\", f1_bow])\n","\n","# --------------------------\n","# 3. Word2Vec (Skip-gram) + LSTM\n","# --------------------------\n","sentences = [text.split() for text in X]\n","w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","sequences = tokenizer.texts_to_sequences(X)\n","word_index = tokenizer.word_index\n","X_seq = pad_sequences(sequences, maxlen=100)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n","\n","model_w2v = Sequential()\n","model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n","\n","from tensorflow.keras.layers import Bidirectional\n","model_w2v.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n","model_w2v.add(Dense(64, activation='relu'))\n","model_w2v.add(Dropout(0.3))\n","\n","model_w2v.add(Dense(3, activation='softmax'))\n","model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n","f1_w2v = macro_f1(y_test, y_pred)\n","results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n","\n","# --------------------------\n","# 4. GloVe (100d) + LSTM\n","# --------------------------\n","glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n","glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    if word in glove_model:\n","        embedding_matrix[i] = glove_model[word]\n","\n","model_glove = Sequential()\n","model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n","\n","from tensorflow.keras.layers import Bidirectional\n","model_glove.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n","model_glove.add(Dense(64, activation='relu'))\n","model_glove.add(Dropout(0.3))\n","\n","model_glove.add(Dense(3, activation='softmax'))\n","model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n","model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n","\n","y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n","f1_glove = macro_f1(y_test, y_pred)\n","results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n","\n","# --------------------------\n","# 5. Comparison Table\n","# --------------------------\n","df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n","print(df_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kO7cSDdtmF5E","executionInfo":{"status":"ok","timestamp":1755074833283,"user_tz":-330,"elapsed":144396,"user":{"displayName":"Mr. Ashwani Balyan (SU Training Delivery Manager)","userId":"10495710902595717956"}},"outputId":"55c955f1-97dc-4b5b-aeaf-c864902b19b8"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 457ms/step - loss: 0.7510\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 323ms/step - loss: 0.4960\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 338ms/step - loss: 0.5186\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 333ms/step - loss: 0.5092\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 315ms/step - loss: 0.4643\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 329ms/step - loss: 0.7298\n","Epoch 2/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 277ms/step - loss: 0.5546\n","Epoch 3/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 298ms/step - loss: 0.5311\n","Epoch 4/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 333ms/step - loss: 0.4439\n","Epoch 5/5\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 324ms/step - loss: 0.4198\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step\n","  Representation Technique Model Type  Macro F1 Score\n","0                      BOW     GBoost        0.539407\n","1     Word2Vec (Skip-gram)       LSTM        0.309333\n","2             GloVe (100d)       LSTM        0.418399\n","3                      BOW     GBoost        0.571731\n","4     Word2Vec (Skip-gram)       LSTM        0.384235\n","5             GloVe (100d)       LSTM        0.343494\n","6                      BOW     GBoost        0.573488\n","7     Word2Vec (Skip-gram)       LSTM        0.310160\n","8             GloVe (100d)       LSTM        0.482493\n"]}]}]}